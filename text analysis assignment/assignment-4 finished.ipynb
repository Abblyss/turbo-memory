{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analytics I HWS 22/23\n",
    "\n",
    "# Home Assignment 4 (29 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit your solution via Ilias until 23.59pm on Tuesday, November 29th. Late submissions are accepted until 10:15am on the following day (start of the exercise), with 1/4 of the total possible points deducted from the score.\n",
    "\n",
    "Submit your solutions in teams of 3-4 students. Unless explicitly agreed otherwise in advance, **submissions from teams with more or less members will NOT be graded**.\n",
    "List all members of the team with their student ID and full name in the cell below, and submit only one notebook per team. Only submit a notebook, do not submit the dataset(s) you used or image files that you have created - these have to be created from your notebook. Also, do NOT compress/zip your submission!\n",
    "\n",
    "You may use the code from the exercises and basic functionalities that are explained in the official documentation of Python packages without citing, __all other sources must be cited__. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members will be expelled from the course without warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General guidelines:\n",
    "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points.\n",
    "* Use only packages that are automatically installed along with Anaconda, plus some additional packages that have been introduced in the context of this class.\n",
    "* Ensure that the notebook does not rely on the current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
    "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Do not reuse variable names multiple times!\n",
    "* Ensure your code/notebook terminates in reasonable time.\n",
    "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
    "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
    "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
    "* If you have any general question regarding the understanding of some task, do not hesitate to post in the student forum in Ilias, so we can clear up such questions for all students in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members\n",
    "team_members = [\n",
    "   {\n",
    "        'first_name': 'Qi',\n",
    "        'last_name': 'Jiang',\n",
    "        'student_id': 1820722\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Jiatong',\n",
    "        'last_name': 'Yu',\n",
    "        'student_id': 1908029\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Yongyi',\n",
    "        'last_name': 'Zheng',\n",
    "        'student_id': 1823267\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Zihang',\n",
    "        'last_name': 'Chen',\n",
    "        'student_id': 1819574\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Set, Tuple\n",
    "from numpy.typing import NDArray\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Term Frequency - Inverse Document Frequency (29 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we want to use the term frequency - inverse document frequency (tf-idf) weighting method to compare documents with each other and to queries. In the end, we will apply our method to a subset of wikipedia pages (more specifically: only the introduction sections) that are linked to from the English Wikipedia page of Mannheim.\n",
    "\n",
    "In case you need to tokenize any sentences in the following tasks, please use a tokenizer from NLTK and not the ``string.split`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a)__ To test your implementation throughout this task, you are given the example from exercise 8. Start by implementing a function ``process_docs`` that takes the provided dictionary of documents and returns the following data structures. __(4 pts)__\n",
    "\n",
    "- ``word2index``: a dictionary that maps each word that appears in any document to a unique integer identifier starting at 0 \n",
    "- ``doc2index``: a dictionary that maps each document name (here given as the dictionary keys) to a unique integer identifier starting at 0\n",
    "- ``index2doc``: a dictionary that maps each document identifier to the corresponding document name (reverse to ``doc2index``)\n",
    "- ``doc_word_vectors``: a dictionary that maps each document name to a list of word ids that indicate which words appeared in the document in their order of appearance. Words that appear multiple times must also be included multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from exercise 8\n",
    "d1 = \"cold beer beach\"\n",
    "d2 = \"ice cream beer beer\"\n",
    "d3 = \"beach cold ice cream\"\n",
    "d4 = \"cold beer frozen yogurt frozen beer\"\n",
    "d5 = \"frozen ice ice beer ice cream\"\n",
    "d6 = \"yogurt ice cream ice cream\"\n",
    "\n",
    "docs = {\"d1\": d1, \"d2\": d2, \"d3\": d3, \"d4\": d4, \"d5\": d5, \"d6\": d6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def process_docs(docs: Dict[str, str]) -> (Dict[str, int], Dict[str, int], Dict[int, str], Dict[str, List[int]]):\n",
    "    \"\"\"\n",
    "    :params docs: dict that maps each document name to the document content\n",
    "    :returns:\n",
    "        - word2index: dict that maps each word to a unique id\n",
    "        - doc2index: dict that maps each document name to a unique id\n",
    "        - index2doc: dict that maps ids to their associated document name\n",
    "        - doc_word_vectors: dict that maps each document name to a list of word ids that appear in it\n",
    "    \"\"\"\n",
    "    word2index = dict()\n",
    "    doc2index = dict()\n",
    "    index2doc = dict()\n",
    "    doc_word_vectors = dict()\n",
    "    word_index = 0\n",
    "    doc_index = 0\n",
    "    \n",
    "    for doc_name in docs.keys():\n",
    "        \n",
    "        # to generate word2index\n",
    "        sentence = word_tokenize(docs[doc_name])\n",
    "        for word in sentence:\n",
    "            if word not in word2index.keys():\n",
    "                word2index[word] = word_index\n",
    "                word_index += 1\n",
    "                \n",
    "        # to generate doc2index and index2doc\n",
    "        if doc_name not in doc2index.keys():\n",
    "            doc2index[doc_name] = doc_index\n",
    "            index2doc[doc_index] = doc_name\n",
    "            doc_index +=1\n",
    "    \n",
    "    # to generate doc_word_vectors    \n",
    "    for doc_name in docs.keys():\n",
    "        sentence = word_tokenize(docs[doc_name])\n",
    "        list_of_ids = [word2index[word] for word in sentence]\n",
    "        doc_word_vectors[doc_name] = list_of_ids\n",
    "       \n",
    "    return word2index, doc2index, index2doc, doc_word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'cold': 0, 'beer': 1, 'beach': 2, 'ice': 3, 'cream': 4, 'frozen': 5, 'yogurt': 6}, {'d1': 0, 'd2': 1, 'd3': 2, 'd4': 3, 'd5': 4, 'd6': 5}, {0: 'd1', 1: 'd2', 2: 'd3', 3: 'd4', 4: 'd5', 5: 'd6'}, {'d1': [0, 1, 2], 'd2': [3, 4, 1, 1], 'd3': [2, 0, 3, 4], 'd4': [0, 1, 5, 6, 5, 1], 'd5': [5, 3, 3, 1, 3, 4], 'd6': [6, 3, 4, 3, 4]})\n"
     ]
    }
   ],
   "source": [
    "# The output for the provided example could look like this:\n",
    "\n",
    "# word2index:\n",
    "# {'cold': 0, 'beer': 1, 'beach': 2, 'ice': 3, 'cream': 4, 'frozen': 5, 'yogurt': 6}\n",
    "\n",
    "# doc2index:\n",
    "# {'d1': 0, 'd2': 1, 'd3': 2, 'd4': 3, 'd5': 4, 'd6': 5}\n",
    "\n",
    "# index2doc\n",
    "# {0: 'd1', 1: 'd2', 2: 'd3', 3: 'd4', 4: 'd5', 5: 'd6'}\n",
    "\n",
    "# doc_word_vectors:\n",
    "# {'d1': [0, 1, 2],\n",
    "#  'd2': [3, 4, 1, 1],\n",
    "#  'd3': [2, 0, 3, 4],\n",
    "#  'd4': [0, 1, 5, 6, 5, 1],\n",
    "#  'd5': [5, 3, 3, 1, 3, 4],\n",
    "#  'd6': [6, 3, 4, 3, 4]}\n",
    "print(process_docs(docs))\n",
    "word2index, doc2index, index2doc, doc_word_vectors=process_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b)__ Set up a term-document matrix where each column corresponds to a document and each row corresponds to a word that was observed in any of the documents. The row/column indices should correspond to the word/document ids that are set in the input dicts ``word2index`` and ``doc2index``. Count how often each word appears in each document and fill the term document matrix. __(3 pts)__\n",
    "\n",
    "_Example: The word \"beer\" with the word id_ ``1`` _appears two times in the document \"d4\" that has the document id_ ``3``. _Therefore the the entry at position_ ``[1, 3]`` _in the term-document matrix is_ ``2``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_document_matrix(doc_word_v: Dict[str, List[int]], doc2index: Dict[str, int], word2index: Dict[str, int]) -> NDArray[NDArray[float]]:\n",
    "    \"\"\"\n",
    "    :param doc_word_v: dict that maps each document to the list of word ids that appear in it\n",
    "    :param doc2index: dict that maps each document name to a unique id\n",
    "    :param word2index: dict that maps each word to a unique id\n",
    "    :return: term-document matrix (each word is a row, each document is a column) that indicates the count of each word in each document \n",
    "    \"\"\"\n",
    "    term_document = np.zeros((len(word2index), len(doc2index)))\n",
    "    for k,v in doc_word_v.items():\n",
    "        for i in v:\n",
    "            term_document[i,doc2index[k]]+=1\n",
    "    return term_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 1. 0. 0.]\n",
      " [1. 2. 0. 2. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 3. 2.]\n",
      " [0. 1. 1. 0. 1. 2.]\n",
      " [0. 0. 0. 2. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "td_matrix = term_document_matrix(doc_word_vectors,doc2index,word2index)\n",
    "print(td_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c)__ Implement the function ``to_tf_idf_matrix`` that takes a term-document matrix and returns the corresponding term frequency (tf) matrix. If the parameter ``idf`` is set to ``True``, the tf-matrix should further be transformed to a tf-idf matrix (i.e. every entry corresponds to the tf-idf value of the associated word and document). Your implementation should leave the input term-document matrix unchanged. __(3 pts)__\n",
    "\n",
    "Recall the following formulas:\n",
    "\n",
    "$\\begin{equation}\n",
    "  tf_{t,d} =\n",
    "    \\begin{cases}\n",
    "      1+log_{10}\\text{count}(t,d) & \\text{if count$(t, d) > 0$}\\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}       \n",
    "\\end{equation}$  \n",
    "\n",
    "$idf_t = log_{10}(\\frac{N}{df_i}) $  \n",
    "\n",
    "$tf-idf_{t,d} = tf_{t,d} \\cdot idf_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf_idf_matrix(td_matrix: NDArray[NDArray[float]], idf: bool=True) -> NDArray[NDArray[float]]:\n",
    "    \"\"\"\n",
    "    :param td_matrix: term-document matrix \n",
    "    :param idf: computes the tf-idf matrix if True, otherwise computes only the tf matrix\n",
    "    :return: matrix with tf(-idf) values for each word-document pair \n",
    "    \"\"\"\n",
    "    tf_matrix = np.where(td_matrix > 0, 1 + np.log10(td_matrix), 0)\n",
    "    if idf:\n",
    "        N = td_matrix.shape[1]\n",
    "        idf = []\n",
    "        for i in range(td_matrix.shape[0]):\n",
    "            idf.append(np.log10(N/sum(np.where(td_matrix[i] > 0, 1, 0))))\n",
    "            for j in range(td_matrix.shape[1]):\n",
    "                tf_matrix[i,j] *= idf[i]\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.30103   , 0.        , 1.30103   , 1.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.        , 1.        , 0.        , 1.47712125,\n",
       "        1.30103   ],\n",
       "       [0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.30103   ],\n",
       "       [0.        , 0.        , 0.        , 1.30103   , 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tf_idf_matrix(td_matrix, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30103   , 0.        , 0.30103   , 0.30103   , 0.        ,\n",
       "        0.        ],\n",
       "       [0.17609126, 0.22910001, 0.        , 0.22910001, 0.17609126,\n",
       "        0.        ],\n",
       "       [0.47712125, 0.        , 0.47712125, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.17609126, 0.17609126, 0.        , 0.26010814,\n",
       "        0.22910001],\n",
       "       [0.        , 0.17609126, 0.17609126, 0.        , 0.17609126,\n",
       "        0.22910001],\n",
       "       [0.        , 0.        , 0.        , 0.62074906, 0.47712125,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.47712125, 0.        ,\n",
       "        0.47712125]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tf_idf_matrix(td_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d)__ We want to test our implementation on our running example. First, print the tf-idf for each word of the query ``ice beer`` with respect to each document. Second, find the two most similar documents from ``d1, d2, d3`` according to cosine similarity and print all similarity values.  __(3 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF values for ice:\n",
      "[0.         0.17609126 0.17609126 0.         0.26010814 0.22910001]\n",
      "TF-IDF values for beer:\n",
      "[0.17609126 0.22910001 0.         0.22910001 0.17609126 0.        ]\n",
      "Cosine similarity of d1 & d2:\n",
      "0.201730941334603\n",
      "Cosine similarity of d1 & d3:\n",
      "0.8732802004950573\n",
      "Cosine similarity of d2 & d3:\n",
      "0.2971975761024058\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "word2index,doc2index,index2doc,doc_word_vectors=process_docs(docs)\n",
    "td_matrix = term_document_matrix(doc_word_vectors,doc2index,word2index)\n",
    "tf_idf = to_tf_idf_matrix(td_matrix)\n",
    "query = (\"ice\",\"beer\")\n",
    "for q in query:\n",
    "    print(\"TF-IDF values for \"+q+\":\")\n",
    "    print(tf_idf[word2index[q]])\n",
    "# ice = tf_idf[3]\n",
    "# beer = tf_idf[1]\n",
    "\n",
    "def cosine_sim(x, y):\n",
    "    return np.dot(x, y) / (norm(x) * norm(y))\n",
    "\n",
    "print(\"Cosine similarity of d1 & d2:\")\n",
    "print(cosine_sim(tf_idf[:,0], tf_idf[:,1]))\n",
    "print(\"Cosine similarity of d1 & d3:\")\n",
    "print(cosine_sim(tf_idf[:,0], tf_idf[:,2]))\n",
    "print(\"Cosine similarity of d2 & d3:\")\n",
    "print(cosine_sim(tf_idf[:,1], tf_idf[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e)__ In a second step we want to find the documents that are most similar to a provided query. Therefore, implement the function ``process_query`` that creates a vector represention of the query. __(5 pts)__\n",
    "\n",
    "First, apply stemming to each query word. Second, create a vector that has an entry for each vocabulary word (words that appeared in any document), where the entry at position ``i`` indicates how often the word with id ``i`` (as indicated by ``word2index``) appears in the query. \n",
    "\n",
    "If ``tf`` is set to ``True``, you should transform all entries to tf-values. Similar, if ``idf`` is set to ``True``, return a vector with tf-idf values (cf. task __c)__). The computation of the idf values is based on the corresponding input term-document matrix.\n",
    "\n",
    "In case the query contains words that are not in any of the documents, print an appropriate error message and stop the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def process_query(query: List[str], word2index: Dict[str, int], td_matrix: NDArray[NDArray[float]], tf: bool=True, idf: bool=True) -> NDArray[float]:\n",
    "    \"\"\"\n",
    "    :param query: list of query tokens\n",
    "    :param word2index: dict that maps each word to a unique id\n",
    "    :param td_matrix: term-document matrix\n",
    "    :param tf: computes the tf vector of the query if True\n",
    "    :param idf: computes the tf-idf vector of the query if True, ignored if tf=False\n",
    "    :return: vector representation of the input query (using tf(-idf))    \n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_q = [stemmer.stem(word) for word in query]\n",
    "    result = np.zeros(len(word2index))\n",
    "    for q in stemmed_q:\n",
    "        if q not in word2index:\n",
    "            raise Exception(f\"The word {q} was not found any document.\")\n",
    "            \n",
    "        else:\n",
    "            result[word2index[q]]+=1\n",
    "    if tf:\n",
    "        result = np.where(result > 0, 1 + np.log10(result), 0)\n",
    "    if idf:\n",
    "        N = td_matrix.shape[1]\n",
    "        for i in range(td_matrix.shape[0]):\n",
    "            result[i]*=np.log10(N/sum(np.where(td_matrix[i] > 0, 1, 0)))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 1. 0. 0. 0.]\n",
      "[0. 1. 0. 1. 0. 0. 0.]\n",
      "[0.         0.17609126 0.         0.17609126 0.         0.\n",
      " 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "test = [\"ice\", \"beer\"]\n",
    "\n",
    "print(process_query(test, word2index, td_matrix,False,False))\n",
    "print(process_query(test, word2index, td_matrix,True,False))\n",
    "print(process_query(test, word2index, td_matrix,True,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__f)__ Implement a function ``most_similar_docs`` that gets the vector representation of a query (in terms of counts, tf values or tf-idf values) and a term-document matrix that can either contain counts, tf-values or tf-idf values.  Compute the cosine similarity between the query and all documents and return the document names and the cosine similarity values of the top-``k`` documents that are most similar to the query. The value of ``k`` should be specified by the user. __(3 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_docs(query_v: NDArray[float], td_matrix: NDArray[NDArray[float]], index2doc: Dict[int, str], k: int) -> (List[str], List[float]):\n",
    "    \"\"\"\n",
    "    :param query_v: vector representation of the input query\n",
    "    :param td_matrix: term-document matrix, possibly with tf-(idf) values \n",
    "    :param index2doc: dict that maps each document id to its name\n",
    "    :k: number of documents to return\n",
    "    :returns:\n",
    "        - list with names of the top-k most similar documents to the query, ordered by descending similarity\n",
    "        - list with cosine similarities of the top-k most similar docs, ordered by descending similarity\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    if k> len(index2doc):\n",
    "        k = len(index2doc)\n",
    "    d = {}\n",
    "    for i in range(len(index2doc)):\n",
    "        sim = (cosine_sim(query_v, td_matrix[:,i]))\n",
    "        d[index2doc[i]] = sim\n",
    "    result = sorted(d.items(), key = lambda x:x[1], reverse=True)\n",
    "    tmp1 = []\n",
    "    tmp2 = []\n",
    "    for i in range(k):\n",
    "        tmp1.append(list(result[i])[0])\n",
    "        tmp2.append(list(result[i])[1])\n",
    "    return tmp1,tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['d3', 'd1', 'd2'], [1.0, 0.8732802004950573, 0.2971975761024058])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "td = to_tf_idf_matrix(td_matrix)\n",
    "test = ['beach', 'cold', 'ice', 'cream']\n",
    "q = process_query(test, word2index, td_matrix,True,True)\n",
    "most_similar_docs(q,td,index2doc,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__g)__ Finally, apply your implementation to a subset of Wikipedia articles that are linked to from the English Wikipedia page of \"Mannheim\". __(5 + 2 + 2 + 2 pts)__\n",
    "\n",
    "The file docs.zip contains the introduction sections of 255 Wikipedia articles, where the file names indicate\n",
    "the names of the corresponding Wikipedia pages. Each file correponds to a document, where stopwords have already been removed and stemming and conversion to lower-case has been applied. Besides tokenization there is no more cleaning needed.\n",
    "\n",
    "Read each file and create the same data structures as in task __a)__. To do this, consider the Python package ``os`` (https://docs.python.org/3/library/os.html) and specifically the function ``os.walk``. When opening and reading the files, make sure to set the encoding to ``utf-8``.\n",
    "\n",
    "Make use of the functions that you implemented in the course of this assignment to compute and print the 5 most similar documents and their cosine similarity for the query ``mannheim university`` by considering\n",
    "\n",
    "i. only word counts  \n",
    "\n",
    "ii. tf values\n",
    "\n",
    "iii. tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_wiki = [\"university\", \"city\", \"ice\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007 FEI European Jumping Championship\n",
      "3 Liga\n",
      "44th Infantry Division (United States)\n",
      "7th Signal Brigade (United States)\n",
      "Aachen\n",
      "ABB\n",
      "Adler Mannheim\n",
      "Aerobus\n",
      "Air pollution\n",
      "Alb-Donau-Kreis\n",
      "Albert Speer\n",
      "Alliance 90 The Greens\n",
      "Alliance for Innovation and Justice\n",
      "Alsace\n",
      "Alstom\n",
      "Alternative for Germany\n",
      "American football\n",
      "American Forces Network\n",
      "Arena\n",
      "Art Nouveau\n",
      "Augsburg\n",
      "August von Kotzebue\n",
      "Augustaanlage\n",
      "Bad D乺kheim\n",
      "Bad Rappenau\n",
      "Baden-Baden\n",
      "Baden-W乺ttemberg\n",
      "Baden-W乺ttemberg Cooperative State University\n",
      "Baroque architecture\n",
      "Basel\n",
      "BASF\n",
      "Battalion\n",
      "Benjamin Franklin Village\n",
      "Bergisch Gladbach\n",
      "Berlin\n",
      "Bertha Benz\n",
      "Bertha Benz Memorial Route\n",
      "Biberach (district)\n",
      "Bicycle\n",
      "Bielefeld\n",
      "Bild\n",
      "Bochum\n",
      "Bodenseekreis\n",
      "Bombardier Inc\n",
      "Bombing of Mannheim in World War II\n",
      "Bonn\n",
      "Bottrop\n",
      "Braunschweig\n",
      "Breisgau-Hochschwarzwald\n",
      "Bremen\n",
      "Bremerhaven\n",
      "Buchen\n",
      "Bundesgartenschau\n",
      "Bundesliga\n",
      "Bundesliga (baseball)\n",
      "Business administration\n",
      "Bydgoszcz\n",
      "B乴ent Ceylan\n",
      "B攂lingen (district)\n",
      "Calw (district)\n",
      "Carl Benz\n",
      "Carl-Benz-Stadion\n",
      "Carlo Grua\n",
      "Caterpillar Inc\n",
      "Central European Summer Time\n",
      "Central European Time\n",
      "Charlottenburg-Wilmersdorf\n",
      "Chemnitz\n",
      "Chernivtsi\n",
      "Chisinau\n",
      "Christian Democratic Union of Germany\n",
      "Christian W攔ns\n",
      "Christiane Schmidtmer\n",
      "Christine Lambrecht\n",
      "Ch僼eau\n",
      "Claus Leininger\n",
      "Cold War\n",
      "Coleman Army Airfield\n",
      "Cologne\n",
      "Creative Cities Network\n",
      "Cumulative voting\n",
      "Darmstadt\n",
      "DAX\n",
      "Daylight saving time\n",
      "Demonym\n",
      "Dennis Siver\n",
      "Deutsche Eishockey Liga\n",
      "Deutscher Wetterdienst\n",
      "Deutschlandfunk\n",
      "Die PARTEI\n",
      "Die Rheinpfalz\n",
      "Die Zeit\n",
      "Districts of Germany\n",
      "Dortmund\n",
      "Draisine\n",
      "Dresden\n",
      "Duisburg\n",
      "D乻seldorf\n",
      "Eberbach (Baden)\n",
      "Edenkoben\n",
      "Eishockey-Bundesliga\n",
      "Electoral Palatinate\n",
      "Electorate of Bavaria\n",
      "Electrical grid\n",
      "Emmendingen (district)\n",
      "Emmy Wehlen\n",
      "Enzkreis\n",
      "Erfurt\n",
      "Erlangen\n",
      "Essen\n",
      "Essity\n",
      "Esslingen (district)\n",
      "European Show Jumping Championships\n",
      "Fernmeldeturm Mannheim\n",
      "Forbes\n",
      "Frankfurt\n",
      "Frankfurt_Mannheim high_speed railway\n",
      "Franz Jung (bishop)\n",
      "Fred Breinersdorfer\n",
      "Frederick IV, Elector Palatine\n",
      "Free Democratic Party (Germany)\n",
      "Freiburg (region)\n",
      "Freiburg im Breisgau\n",
      "French Army\n",
      "Freudenstadt (district)\n",
      "Friedrich Engelhorn\n",
      "Friedrich Schiller\n",
      "Friedrichsplatz\n",
      "Fuchs Petrolub\n",
      "F乺th\n",
      "Gasoline\n",
      "Gelsenkirchen\n",
      "Geographic coordinate system\n",
      "German language\n",
      "German National Library\n",
      "German Wine Route\n",
      "Germanic name\n",
      "Germany\n",
      "Giulia Enders\n",
      "Global city\n",
      "Globalization and World Cities Research Network\n",
      "Google Maps\n",
      "Grand Duchy of Baden\n",
      "Gross domestic product\n",
      "G乼ersloh\n",
      "G攑pingen (district)\n",
      "G攖tingen\n",
      "Hagen\n",
      "Haifa\n",
      "Hakan Calhanoglu\n",
      "Halle (Saale)\n",
      "Hamburg\n",
      "Hamm\n",
      "Hanau\n",
      "Hanover\n",
      "Hans Filbinger\n",
      "Harvard University\n",
      "Hectare\n",
      "Hedwig Hillenga_\n",
      "Heidelberg\n",
      "Heidelberg University\n",
      "Heidelberg University Faculty of Medicine in Mannheim\n",
      "Heidenheim (district)\n",
      "Heilbronn\n",
      "Heilbronn (district)\n",
      "Henry Morgenthau Sr\n",
      "Herne, North Rhine-Westphalia\n",
      "Hesse\n",
      "Hildesheim\n",
      "History of German football\n",
      "Hohenlohe (district)\n",
      "Human Environment Animal Protection\n",
      "IBM\n",
      "Ice hockey\n",
      "Imperial Russian Army\n",
      "Ingolstadt\n",
      "Intercity Express\n",
      "Internal combustion engine\n",
      "International Filmfestival Mannheim-Heidelberg\n",
      "Irreligion\n",
      "Jena\n",
      "Jesuit Church, Mannheim\n",
      "Jochen Hecht\n",
      "Johann Baptist Cramer\n",
      "Josepha von Heydeck\n",
      "Juergen Adams\n",
      "Julius Hatry\n",
      "Kaiserslautern\n",
      "Kapuzinerplanken\n",
      "Karl Drais\n",
      "Karlsruhe\n",
      "Karlsruhe (district)\n",
      "Karlsruhe (region)\n",
      "Kassel\n",
      "Kiel\n",
      "Klaipeda\n",
      "Koblenz\n",
      "Kohlhammer Verlag\n",
      "Konrad Adenauer Bridge\n",
      "Konstanz (district)\n",
      "Krefeld\n",
      "Kunsthalle Mannheim\n",
      "Kurpfalz (region)\n",
      "Kurt Fleckenstein\n",
      "K攑pen climate classification\n",
      "Ladenburg\n",
      "Leipzig\n",
      "Leverkusen\n",
      "Lexi Alexander\n",
      "List of cities and towns in Germany\n",
      "List of cities in Baden-W乺ttemberg by population\n",
      "List of cities in Germany by population\n",
      "List of dialling codes in Germany\n",
      "List of districts of Germany\n",
      "List of German cities by GDP\n",
      "List of municipalities in Germany\n",
      "List of Sch乼te-Lanz airships\n",
      "List of twin towns and sister cities in Germany\n",
      "London\n",
      "Lord mayor\n",
      "Lorraine\n",
      "Lorsch\n",
      "Lorsch Abbey\n",
      "Lorsch codex\n",
      "Ludwigsburg (district)\n",
      "Ludwigshafen\n",
      "Luisenpark\n",
      "L乥eck\n",
      "L攔rach (district)\n",
      "Magdeburg\n",
      "Maimarkt-Turnier Mannheim\n",
      "Main-Tauber-Kreis\n",
      "Mainz\n",
      "Mannheim (disambiguation)\n",
      "Mannheim City Airport\n",
      "Mannheim Harbour\n",
      "Mannheim Hauptbahnhof\n",
      "Mannheim May Market\n",
      "Mannheim Palace\n",
      "Mannheim Rangierbahnhof\n",
      "Mannheim Tornados\n",
      "Mannheim Water Tower\n",
      "Mannheim_Frankfurt railway\n",
      "Mannheim_Karlsruhe_Basel railway\n",
      "MDAX\n",
      "Metropolitan area\n",
      "Metropolitan regions in Germany\n",
      "Moers\n",
      "Mosbach\n",
      "Munich\n",
      "MVV Energie\n",
      "M乭lburg\n",
      "M乴heim\n",
      "M乶ster\n",
      "M攏chengladbach\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for root, dirs, files in os.walk(\"docs\"):\n",
    "    for name in files:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wikipedia_docs() -> (Dict[str, int], Dict[str, int], Dict[int, str], Dict[str, List[int]]):\n",
    "    \"\"\"\n",
    "    :returns:\n",
    "    - word2index: dict that maps each word to a unique id\n",
    "    - doc2index: dict that maps each document to a unique id\n",
    "    - index2doc: dict that maps ids to their associated document\n",
    "    - doc_word_vectors: dict that maps each document name to a list of word ids that appear in it\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    docs = {}\n",
    "    for root, dirs, files in os.walk(\"docs\"):\n",
    "        for name in files:\n",
    "            cur = os.path.join(\"docs\", name)\n",
    "            with open(cur,encoding='utf-8') as f:\n",
    "                docs[name]=f.read()\n",
    "    return process_docs(docs)\n",
    "\n",
    "word2index, doc2index, index2doc, doc_word_vectors=process_wikipedia_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Heidelberg University Faculty of Medicine in Mannheim',\n",
       "  'Mannheim (disambiguation)',\n",
       "  'Mannheim Palace',\n",
       "  'Augustaanlage',\n",
       "  'Kunsthalle Mannheim'],\n",
       " [0.40931560807732065,\n",
       "  0.39223227027636803,\n",
       "  0.34503277967117707,\n",
       "  0.3086066999241838,\n",
       "  0.29138575870717925])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i)\n",
    "query_wiki =['mannheim', 'university']\n",
    "q = process_query(query_wiki, word2index, td_matrix,False,False)\n",
    "td_matrix = term_document_matrix(doc_word_vectors,doc2index,word2index)\n",
    "most_similar_docs(q,td_matrix,index2doc,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mannheim (disambiguation)',\n",
       "  'Mannheim Palace',\n",
       "  'Heidelberg University Faculty of Medicine in Mannheim',\n",
       "  'Mannheim Rangierbahnhof',\n",
       "  'Augustaanlage'],\n",
       " [0.281338421216347,\n",
       "  0.2740006740462574,\n",
       "  0.2673707615492943,\n",
       "  0.22421615584111737,\n",
       "  0.212782808600512])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ii)\n",
    "q = process_query(query_wiki, word2index, td_matrix,True,False)\n",
    "td = to_tf_idf_matrix(td_matrix, False)\n",
    "most_similar_docs(q,td,index2doc,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mannheim (disambiguation)',\n",
       "  'Mannheim Palace',\n",
       "  'Heidelberg University Faculty of Medicine in Mannheim',\n",
       "  'Mannheim Rangierbahnhof',\n",
       "  'Bombing of Mannheim in World War II'],\n",
       " [0.17837437258082361,\n",
       "  0.1431133714249375,\n",
       "  0.12457000565595781,\n",
       "  0.11757716481970117,\n",
       "  0.10974076404633058])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iii)\n",
    "q = process_query(query_wiki, word2index, td_matrix,True,True)\n",
    "td = to_tf_idf_matrix(td_matrix, True)\n",
    "most_similar_docs(q,td,index2doc,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
